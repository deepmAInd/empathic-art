{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "Running this preprocessing notebook will result in creation of black and white images of the audio files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We set up the properties of the output and import all libraries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "AUDIO_LENGTH = 5\n",
    "IMG_SIZE = (224, 224)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "from skimage.util import img_as_ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "Next, we define emotions which we'll include in the dataset and create the output folders."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = f'../../data/prepared/mel-spectrogram/pad/prepared_images_{IMG_SIZE[0]}'\n",
    "\n",
    "EMOTIONS = ['happy', 'surprise', 'anger', 'sad', 'neutral', 'disgust', 'fear']\n",
    "EMOTIONS_MAP = {\n",
    "    'happy': 0,\n",
    "    'surprise': 1,\n",
    "    'anger': 2,\n",
    "    'sad': 3,\n",
    "    'neutral': 4,\n",
    "    'disgust': 5,\n",
    "    'fear': 6\n",
    "}\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "for emotion in EMOTIONS:\n",
    "    if not os.path.exists(f'{OUTPUT_FOLDER}/{emotion}'):\n",
    "        os.mkdir(f'{OUTPUT_FOLDER}/{emotion}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read the reference dataset with all the metadata of the audio files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           source dataname    speaker  \\\n0  https://zenodo.org/record/4066235#.Yz_WRNJBwUF    vivae  vivae_S10   \n1  https://zenodo.org/record/4066235#.Yz_WRNJBwUF    vivae  vivae_S02   \n2  https://zenodo.org/record/4066235#.Yz_WRNJBwUF    vivae  vivae_S08   \n3  https://zenodo.org/record/4066235#.Yz_WRNJBwUF    vivae  vivae_S05   \n4  https://zenodo.org/record/4066235#.Yz_WRNJBwUF    vivae  vivae_S08   \n\n       emotion intensity  duration  samplerate gender statement  \\\n0  achievement    strong         1       44100    NaN       NaN   \n1         pain    strong         1       44100    NaN       NaN   \n2     surprise       low         1       44100    NaN       NaN   \n3        anger      peak         1       44100    NaN       NaN   \n4        anger  moderate         1       44100    NaN       NaN   \n\n                        filename  \n0  S10_achievement_strong_01.wav  \n1         S02_pain_strong_05.wav  \n2        S08_surprise_low_02.wav  \n3          S05_anger_peak_02.wav  \n4      S08_anger_moderate_07.wav  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>dataname</th>\n      <th>speaker</th>\n      <th>emotion</th>\n      <th>intensity</th>\n      <th>duration</th>\n      <th>samplerate</th>\n      <th>gender</th>\n      <th>statement</th>\n      <th>filename</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://zenodo.org/record/4066235#.Yz_WRNJBwUF</td>\n      <td>vivae</td>\n      <td>vivae_S10</td>\n      <td>achievement</td>\n      <td>strong</td>\n      <td>1</td>\n      <td>44100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>S10_achievement_strong_01.wav</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://zenodo.org/record/4066235#.Yz_WRNJBwUF</td>\n      <td>vivae</td>\n      <td>vivae_S02</td>\n      <td>pain</td>\n      <td>strong</td>\n      <td>1</td>\n      <td>44100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>S02_pain_strong_05.wav</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://zenodo.org/record/4066235#.Yz_WRNJBwUF</td>\n      <td>vivae</td>\n      <td>vivae_S08</td>\n      <td>surprise</td>\n      <td>low</td>\n      <td>1</td>\n      <td>44100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>S08_surprise_low_02.wav</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://zenodo.org/record/4066235#.Yz_WRNJBwUF</td>\n      <td>vivae</td>\n      <td>vivae_S05</td>\n      <td>anger</td>\n      <td>peak</td>\n      <td>1</td>\n      <td>44100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>S05_anger_peak_02.wav</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://zenodo.org/record/4066235#.Yz_WRNJBwUF</td>\n      <td>vivae</td>\n      <td>vivae_S08</td>\n      <td>anger</td>\n      <td>moderate</td>\n      <td>1</td>\n      <td>44100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>S08_anger_moderate_07.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('../../data/reference_df.csv')\n",
    "df_.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check which emotions are present in the dataset and keep only needed once."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['achievement' 'anger' 'disgust' 'fear' 'happy' 'neutral' 'pain'\n",
      " 'pleasure' 'sad' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(df_.emotion))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = df_[df_.emotion.isin(EMOTIONS)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define preprocessing functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def spectrogram_image(data, path):\n",
    "    mels = librosa.feature.melspectrogram(data)\n",
    "    mels = np.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "    # min-max scale to fit inside 8-bit range\n",
    "    img = scale_minmax(mels, 0, 255).astype(np.uint8)\n",
    "    img = np.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img # invert. make black==more energy\n",
    "    img = img_as_ubyte(img)\n",
    "    img = resize(img, IMG_SIZE)\n",
    "\n",
    "    # save as PNG\n",
    "    skimage.io.imsave(path, img)\n",
    "\n",
    "def convert_to_image(audio_path, samplerate, save_path):\n",
    "\n",
    "    # Loading the audio file\n",
    "    x, sr = librosa.load(audio_path, sr = None)\n",
    "\n",
    "    # Normalizing +5.0db, transform audio signals to an array\n",
    "    normalized_sound=librosa.util.normalize(x, norm=5)\n",
    "\n",
    "    # Trimming the silence in the beginning and end\n",
    "    xt, index = librosa.effects.trim(normalized_sound, top_db = 30)\n",
    "    padded_x = np.pad(xt, (0, samplerate * AUDIO_LENGTH - len(xt)), 'constant')\n",
    "\n",
    "    # Saving as image\n",
    "    spectrogram_image(padded_x, save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocess all audio files and save them as mel spectrograms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "%%capture\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    data_path = ''\n",
    "    if row.dataname == 'tess':\n",
    "        data_path = 'utoronto/data'\n",
    "    elif row.dataname == 'vivae':\n",
    "        data_path = 'VIVAE/core_set'\n",
    "    elif row.dataname == 'ravdess':\n",
    "        data_path = f'RAVDESS/Audio_Speech_Actors/Actor_{str(row.speaker).split(\"_\")[1]}'\n",
    "\n",
    "    audio_path = f'../../data/{data_path}/{row.filename}'\n",
    "    convert_to_image(audio_path, row.samplerate, f'{OUTPUT_FOLDER}/{row.emotion}/{index}.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
