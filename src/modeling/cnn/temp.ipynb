{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/.local/share/virtualenvs/empathic-art-VdJ2KSTr/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from torch.utils.data.dataloader import Dataset, T_co\n",
    "\n",
    "import noisereduce as nr\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2), # out: 32 x 128 x 128\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2, 2), # out: 64 x 53 x 53\n",
    "\n",
    "            nn.Dropout(p=.4),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(in_features=64 * 53 * 53, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self, xb: torch.Tensor):\n",
    "        return self.network(xb)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images) # generate predictions\n",
    "        loss = functional.cross_entropy(out, labels) # calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images) # generate predictgions\n",
    "        loss = functional.cross_entropy(out, labels) # calculate loss\n",
    "        acc = accuracy(out, labels) # calculate accuracy\n",
    "        return {\"val_loss\": loss.detach(), \"val_acc\": acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "\n",
    "        batch_accs = [x[\"val_acc\"] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {\"val_loss\": epoch_loss.item(), \"val_acc\": epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f\"Epoch {epoch}, train_loss: {result['train_loss']}, \\\n",
    "        val_loss: {result['val_loss']}, val_acc: {result['val_acc']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmotionCNN()\n",
    "model.load_state_dict(torch.load(\"./models/specgram_model.zip\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionCNN(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (5): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU()\n",
       "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (10): Dropout(p=0.4, inplace=False)\n",
       "    (11): Flatten(start_dim=1, end_dim=-1)\n",
       "    (12): Linear(in_features=179776, out_features=512, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmotionCNN(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (5): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (10): Dropout(p=0.4, inplace=False)\n",
      "    (11): Flatten(start_dim=1, end_dim=-1)\n",
      "    (12): Linear(in_features=179776, out_features=512, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (15): ReLU()\n",
      "    (16): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empathic-art-VdJ2KSTr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e50c87813b58f42c33168ff8a825bf8dc8f12ff2146cb51a790b6579912f82a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
